From 4ae3c33dd1ccff13bec57ad335446f4b1398341e Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Tue, 22 Feb 2022 19:31:30 +0000
Subject: [PATCH 85/97] KVM: arm64: SMMUv3: Implement TLB flush callbacks

When the hypervisor updates the host stage-2 page tables, send the TLB
invalidation to each SMMU.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c | 61 +++++++++++++++++++--
 1 file changed, 55 insertions(+), 6 deletions(-)

diff --git a/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c b/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
index d8a569be6354..686f36f84bec 100644
--- a/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
+++ b/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
@@ -136,20 +136,27 @@ static int smmu_sync_cmd(struct hyp_arm_smmu_v3_device *smmu)
 	return smmu_poll_cond(smmu_cmdq_empty(smmu));
 }
 
-static int smmu_sync_ste(struct hyp_arm_smmu_v3_device *smmu, u32 sid)
+static int smmu_send_cmd(struct hyp_arm_smmu_v3_device *smmu,
+			 struct arm_smmu_cmdq_ent *cmd)
 {
 	int ret;
+
+	ret = smmu_add_cmd(smmu, cmd);
+	if (ret)
+		return ret;
+
+	return smmu_sync_cmd(smmu);
+}
+
+static int smmu_sync_ste(struct hyp_arm_smmu_v3_device *smmu, u32 sid)
+{
 	struct arm_smmu_cmdq_ent cmd = {
 		.opcode = CMDQ_OP_CFGI_STE,
 		.cfgi.sid = sid,
 		.cfgi.leaf = true,
 	};
 
-	ret = smmu_add_cmd(smmu, &cmd);
-	if (ret)
-		return ret;
-
-	return smmu_sync_cmd(smmu);
+	return smmu_send_cmd(smmu, &cmd);
 }
 
 static u64 *smmu_get_ste_ptr(struct hyp_arm_smmu_v3_device *smmu, u32 sid)
@@ -528,9 +535,51 @@ static int smmu_dev_set_owner(u64 smmu_base, u32 sid)
 }
 
 
+static int smmu_tlb_flush_vmid(struct kvm_s2_mmu *mmu)
+{
+	struct hyp_arm_smmu_v3_device *smmu;
+	u16 vmid = atomic64_read(&mmu->vmid.id);
+	struct arm_smmu_cmdq_ent cmd = {
+		.opcode = CMDQ_OP_TLBI_S12_VMALL,
+		.tlbi.vmid = vmid,
+	};
+
+	if (vmid != 0)
+		return 0;
+
+	hyp_spin_lock(&smmu_lock);
+	for_each_smmu(smmu)
+		WARN_ON(smmu_send_cmd(smmu, &cmd));
+	hyp_spin_unlock(&smmu_lock);
+	return 0;
+}
+
+static int smmu_tlb_flush_vmid_ipa(struct kvm_s2_mmu *mmu, phys_addr_t ipa,
+				   int level)
+{
+	struct hyp_arm_smmu_v3_device *smmu;
+	u16 vmid = atomic64_read(&mmu->vmid.id);
+	struct arm_smmu_cmdq_ent cmd = {
+		.opcode = CMDQ_OP_TLBI_S2_IPA,
+		.tlbi.vmid = vmid,
+		.tlbi.addr = ipa,
+	};
+
+	if (vmid != 0)
+		return 0;
+
+	hyp_spin_lock(&smmu_lock);
+	for_each_smmu(smmu)
+		WARN_ON(smmu_send_cmd(smmu, &cmd));
+	hyp_spin_unlock(&smmu_lock);
+	return 0;
+}
+
 static struct kvm_iommu_ops smmu_ops = {
 	.init				= smmu_init,
 	.dev_set_owner			= smmu_dev_set_owner,
+	.tlb_flush_vmid			= smmu_tlb_flush_vmid,
+	.tlb_flush_vmid_ipa		= smmu_tlb_flush_vmid_ipa,
 };
 
 int kvm_arm_smmu_v3_register(void)
-- 
2.34.1

