From cfab76c071bdf5c14b1cda9293cf7dbaac5aed2f Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Fri, 18 Feb 2022 14:57:50 +0000
Subject: [PATCH 79/97] KVM: arm64: pkvm: Enable pinning whole host stage-2

Since the host does not tell the hypervisor which memory is used for DMA
at the moment, and devices do not generally support page faults, we must
allow any host memory to be accessed by devices. Pin the host mappings
on initialization, and restore the mappings that the host reclaims when
a guest exits.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/kvm/hyp/include/nvhe/mem_protect.h |  5 ++
 arch/arm64/kvm/hyp/nvhe/Makefile              |  5 ++
 arch/arm64/kvm/hyp/nvhe/mem_protect.c         | 71 ++++++++++++++++---
 arch/arm64/kvm/hyp/nvhe/setup.c               |  8 ++-
 4 files changed, 78 insertions(+), 11 deletions(-)

diff --git a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
index fe37fa811583..e7d72a501f4c 100644
--- a/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
+++ b/arch/arm64/kvm/hyp/include/nvhe/mem_protect.h
@@ -21,6 +21,8 @@
  *       with another entity.
  *   10: The page is shared with, but not owned by the page-table owner.
  *   11: Reserved for future use (lending).
+ *
+ * SW bit 2 is used to pin pages
  */
 enum pkvm_page_state {
 	PKVM_PAGE_OWNED			= 0ULL,
@@ -28,6 +30,7 @@ enum pkvm_page_state {
 	PKVM_PAGE_SHARED_BORROWED	= KVM_PGTABLE_PROT_SW1,
 	__PKVM_PAGE_RESERVED		= KVM_PGTABLE_PROT_SW0 |
 					  KVM_PGTABLE_PROT_SW1,
+	PKVM_PAGE_PINNED		= KVM_PGTABLE_PROT_SW2,
 
 	/* Meta-states which aren't encoded directly in the PTE's SW bits */
 	PKVM_NOPAGE,
@@ -84,6 +87,8 @@ int host_stage2_set_owner_locked(phys_addr_t addr, u64 size, pkvm_id owner_id);
 int host_stage2_unmap_dev_locked(phys_addr_t start, u64 size);
 int kvm_host_prepare_stage2(void *pgt_pool_base);
 int kvm_guest_prepare_stage2(struct kvm_shadow_vm *vm, void *pgd);
+int kvm_host_enable_stage2_pinned(bool skip_bbm);
+int host_stage2_finalize_idmap(u64 vaddr, u32 level);
 struct kvm_s2_mmu *kvm_host_get_stage2(void);
 void handle_host_mem_abort(struct kvm_cpu_context *host_ctxt);
 
diff --git a/arch/arm64/kvm/hyp/nvhe/Makefile b/arch/arm64/kvm/hyp/nvhe/Makefile
index 6f8b69e39d96..c8fec2a221f5 100644
--- a/arch/arm64/kvm/hyp/nvhe/Makefile
+++ b/arch/arm64/kvm/hyp/nvhe/Makefile
@@ -28,6 +28,11 @@ obj-$(CONFIG_KVM_S2MPU) += iommu/s2mpu.o
 
 obj-$(CONFIG_ARM_SMMU_V3_PKVM) += iommu/arm-smmu-v3.o
 
+# In order to access id_aa64mmfr2_el1, older GCC required declaring the arch as
+# armv8.2: https://sourceware.org/bugzilla/show_bug.cgi?id=27139
+# The top Makefile already sets -Wa,-march=8.x-a but it's not sufficient.
+CFLAGS_mem_protect.nvhe.o := -march=armv8.2-a
+
 ##
 ## Build rules for compiling nVHE hyp code
 ## Output of this folder is `kvm_nvhe.o`, a partially linked object
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 69c933a72f72..085985114cad 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -27,8 +27,13 @@
 extern unsigned long hyp_nr_cpus;
 struct host_kvm host_kvm;
 
+/* At the moment we don't differentiate between global and local pinning */
+#define pkvm_host_stage2_is_pinned() (!!host_kvm.pgt.pin_mask)
+
 static struct hyp_pool host_s2_pool;
 
+static bool host_stage2_force_pte;
+
 static pkvm_id pkvm_guest_id(struct kvm_vcpu *vcpu)
 {
 	return vcpu->arch.hw_mmu->vmid.vmid;
@@ -176,6 +181,30 @@ int kvm_host_prepare_stage2(void *pgt_pool_base)
 	return 0;
 }
 
+int kvm_host_enable_stage2_pinned(bool skip_bbm)
+{
+	u64 reg;
+
+	host_kvm.pgt.pin_mask = PKVM_PAGE_PINNED;
+
+	/*
+	 * Because this requires replacing live block mappings with tables, we
+	 * need FEAT_BBM level 1 or greater. In that case we can indeed skip
+	 * break-before-make.
+	 */
+	if (skip_bbm) {
+		reg = read_sysreg(id_aa64mmfr2_el1);
+		reg = (reg >> ID_AA64MMFR2_BBM_SHIFT) & 0xf;
+
+		if (reg >= 1)
+			return 0;
+	}
+
+	/* Otherwise we'll have to use leaf PTEs. */
+	host_stage2_force_pte = true;
+	return 0;
+}
+
 /* The VMID, TTB and TCR must not change while the IOMMU is using them */
 struct kvm_s2_mmu *kvm_host_get_stage2(void)
 {
@@ -561,21 +590,26 @@ int host_stage2_set_owner_locked(phys_addr_t addr, u64 size,
 				 pkvm_id owner_id)
 {
 	kvm_pte_t annotation = kvm_init_invalid_leaf_owner(owner_id);
-	enum kvm_pgtable_prot prot;
-	int ret;
 
-	ret = host_stage2_try(kvm_pgtable_stage2_annotate, &host_kvm.pgt,
-			      addr, size, &host_s2_pool, annotation);
-	if (ret)
-		return ret;
+	/* When pinning is requested, map the page directly */
+	if (owner_id == pkvm_host_id && pkvm_host_stage2_is_pinned()) {
+		enum kvm_pgtable_prot prot = pkvm_mkstate(PKVM_HOST_MEM_PROT,
+							  PKVM_PAGE_OWNED |
+							  PKVM_PAGE_PINNED);
+		pkvm_iommu_host_stage2_idmap(addr, addr + size, prot);
+		return 0;
+	}
 
-	prot = owner_id == pkvm_host_id ? PKVM_HOST_MEM_PROT : 0;
-	pkvm_iommu_host_stage2_idmap(addr, addr + size, prot);
-	return 0;
+	return host_stage2_try(kvm_pgtable_stage2_annotate, &host_kvm.pgt,
+			      addr, size, &host_s2_pool, annotation);
 }
 
 static bool host_stage2_force_pte_cb(u64 addr, u64 end, enum kvm_pgtable_prot prot)
 {
+	if (host_stage2_force_pte)
+		return true;
+
+	prot &= ~PKVM_PAGE_PINNED;
 	/*
 	 * Block mappings must be used with care in the host stage-2 as a
 	 * kvm_pgtable_stage2_map() operation targeting a page in the range of
@@ -674,6 +708,25 @@ static void host_inject_abort(struct kvm_cpu_context *host_ctxt)
 	write_sysreg_el2(spsr, SYS_SPSR);
 }
 
+int host_stage2_finalize_idmap(u64 vaddr, u32 level)
+{
+	size_t granule;
+	phys_addr_t phys;
+	enum kvm_pgtable_prot prot;
+
+	if (!pkvm_host_stage2_is_pinned())
+		return 0;
+
+	phys = __hyp_pa(vaddr);
+	if (!addr_is_memory(phys))
+		return -EINVAL;
+
+	prot = pkvm_mkstate(PKVM_HOST_MEM_PROT,
+			    PKVM_PAGE_OWNED | PKVM_PAGE_PINNED);
+	granule = kvm_granule_size(level);
+	return host_stage2_idmap_locked(phys, granule, prot);
+}
+
 void handle_host_mem_abort(struct kvm_cpu_context *host_ctxt)
 {
 	struct kvm_vcpu_fault_info fault;
diff --git a/arch/arm64/kvm/hyp/nvhe/setup.c b/arch/arm64/kvm/hyp/nvhe/setup.c
index c8db44d10c5c..b111775f9eee 100644
--- a/arch/arm64/kvm/hyp/nvhe/setup.c
+++ b/arch/arm64/kvm/hyp/nvhe/setup.c
@@ -195,8 +195,12 @@ static int fix_host_ownership_walker(u64 addr, u64 end, u32 level,
 	kvm_pte_t pte = *ptep;
 	phys_addr_t phys;
 
-	if (!kvm_pte_valid(pte))
-		return 0;
+	if (!kvm_pte_valid(pte)) {
+		if (flag != KVM_PGTABLE_WALK_LEAF)
+			return 0;
+
+		return host_stage2_finalize_idmap(addr, level);
+	}
 
 	if (level != (KVM_PGTABLE_MAX_LEVELS - 1))
 		return -EINVAL;
-- 
2.34.1

