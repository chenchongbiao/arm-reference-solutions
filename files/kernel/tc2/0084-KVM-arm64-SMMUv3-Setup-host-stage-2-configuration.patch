From dc784a86a3837bb416948f7f12fbaa2f5529d375 Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Thu, 17 Feb 2022 16:33:49 +0000
Subject: [PATCH 84/97] KVM: arm64: SMMUv3: Setup host stage-2 configuration

Handle __pkvm_dev_set_owner() hypercall, by initializing the Stream
Table Entry and enabling translation for the given device.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c | 140 ++++++++++++++++++++
 1 file changed, 140 insertions(+)

diff --git a/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c b/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
index de5d281807e8..d8a569be6354 100644
--- a/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
+++ b/arch/arm64/kvm/hyp/nvhe/iommu/arm-smmu-v3.c
@@ -4,10 +4,12 @@
  *
  * Copyright (C) 2022 Linaro Ltd.
  */
+#include <asm/kvm_host.h>
 #include <asm/kvm_hyp.h>
 #include <asm/kvm_pkvm.h>
 #include <kvm/arm_smmu_v3.h>
 #include <linux/arm_smmu_v3_regs.h>
+#include <nvhe/mem_protect.h>
 #include <nvhe/mm.h>
 
 #define ARM_SMMU_POLL_TIMEOUT_US	1000000 /* 1s! */
@@ -20,6 +22,9 @@ struct hyp_arm_smmu_v3_device __ro_after_init *kvm_hyp_arm_smmu_v3_devices;
 	     (smmu) != &kvm_hyp_arm_smmu_v3_devices[kvm_hyp_arm_smmu_v3_nr_devices]; \
 	     (smmu)++)
 
+/* Serialize access to both stream table and command queue */
+static hyp_spinlock_t smmu_lock;
+
 /*
  * Wait until @cond is true.
  * Return 0 on success, or -ETIMEDOUT
@@ -131,6 +136,115 @@ static int smmu_sync_cmd(struct hyp_arm_smmu_v3_device *smmu)
 	return smmu_poll_cond(smmu_cmdq_empty(smmu));
 }
 
+static int smmu_sync_ste(struct hyp_arm_smmu_v3_device *smmu, u32 sid)
+{
+	int ret;
+	struct arm_smmu_cmdq_ent cmd = {
+		.opcode = CMDQ_OP_CFGI_STE,
+		.cfgi.sid = sid,
+		.cfgi.leaf = true,
+	};
+
+	ret = smmu_add_cmd(smmu, &cmd);
+	if (ret)
+		return ret;
+
+	return smmu_sync_cmd(smmu);
+}
+
+static u64 *smmu_get_ste_ptr(struct hyp_arm_smmu_v3_device *smmu, u32 sid)
+{
+	u32 idx;
+	u64 l1std, span, *base;
+
+	if (sid >= smmu->strtab_num_entries)
+		return NULL;
+
+	if (!smmu->strtab_split)
+		return smmu->strtab_base + sid * STRTAB_STE_DWORDS;
+
+	idx = sid >> smmu->strtab_split;
+	l1std = smmu->strtab_base[idx];
+
+	span = l1std & STRTAB_L1_DESC_SPAN;
+	idx = sid & ((1 << smmu->strtab_split) - 1);
+	if (!span || idx >= (1 << (span - 1)))
+		return NULL;
+
+	base = hyp_phys_to_virt(l1std & STRTAB_L1_DESC_L2PTR_MASK);
+	return base + idx * STRTAB_STE_DWORDS;
+}
+
+static int smmu_enable_ste(struct hyp_arm_smmu_v3_device *smmu, u32 sid,
+			   struct kvm_s2_mmu *mmu)
+{
+	int i;
+	int ret;
+	u16 vmid;
+	u64 vtcr;
+	u64 ent[STRTAB_STE_DWORDS] = {};
+	u64 ts, sl, ic, oc, sh, tg, ps, ha, hd;
+	u64 *dst = smmu_get_ste_ptr(smmu, sid);
+
+	if (!dst)
+		return -ENODEV;
+
+	ts = FIELD_GET(VTCR_EL2_T0SZ_MASK, mmu->arch->vtcr);
+	sl = FIELD_GET(VTCR_EL2_SL0_MASK, mmu->arch->vtcr);
+	ic = FIELD_GET(VTCR_EL2_IRGN0_MASK, mmu->arch->vtcr);
+	oc = FIELD_GET(VTCR_EL2_ORGN0_MASK, mmu->arch->vtcr);
+	sh = FIELD_GET(VTCR_EL2_SH0_MASK, mmu->arch->vtcr);
+	tg = FIELD_GET(VTCR_EL2_TG0_MASK, mmu->arch->vtcr);
+	ps = FIELD_GET(VTCR_EL2_PS_MASK, mmu->arch->vtcr);
+	ha = FIELD_GET(VTCR_EL2_HA, mmu->arch->vtcr);
+	hd = FIELD_GET(VTCR_EL2_HD, mmu->arch->vtcr);
+
+	/*
+	 * If the SMMU doesn't support HTTU, then we disable hardware access and
+	 * dirty updates. In this case, access faults are disabled by the affd
+	 * bit: the access flag is considered to be always 1. The dirty bit
+	 * cannot be handled in the same way. For the moment none of this
+	 * matters, as all PTEs are RW with AF set.
+	 */
+	if (1 /* !(smmu->feat & ARM_SMMU_FEAT_HTTU) TODO */)
+		ha = hd = 0;
+
+	vtcr = FIELD_PREP(STRTAB_STE_2_VTCR_S2T0SZ, ts) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2SL0, sl) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2IR0, ic) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2OR0, oc) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2SH0, sh) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2TG, tg) |
+	       FIELD_PREP(STRTAB_STE_2_VTCR_S2PS, ps);
+
+	/* This VMID is pinned */
+	vmid = atomic64_read(&mmu->vmid.id);
+
+	ent[0] |= STRTAB_STE_0_V;
+	ent[0] |= FIELD_PREP(STRTAB_STE_0_CFG, STRTAB_STE_0_CFG_S2_TRANS);
+	ent[2] |= FIELD_PREP(STRTAB_STE_2_VTCR, vtcr);
+	ent[2] |= FIELD_PREP(STRTAB_STE_2_S2HA, ha);
+	ent[2] |= FIELD_PREP(STRTAB_STE_2_S2HD, hd);
+	ent[2] |= STRTAB_STE_2_S2AFFD;
+	ent[2] |= FIELD_PREP(STRTAB_STE_2_S2VMID, vmid);
+	ent[2] |= STRTAB_STE_2_S2AA64;
+	ent[3] |= mmu->pgd_phys & STRTAB_STE_3_S2TTB_MASK;
+
+	/*
+	 * The SMMU may cache a disabled STE.
+	 * Initialize all fields, sync, then enable it.
+	 */
+	for (i = 1; i < STRTAB_STE_DWORDS; i++)
+		dst[i] = cpu_to_le64(ent[i]);
+
+	ret = smmu_sync_ste(smmu, sid);
+	if (ret)
+		return ret;
+
+	WRITE_ONCE(dst[0], cpu_to_le64(ent[0]));
+	return smmu_sync_ste(smmu, sid);
+}
+
 static int smmu_init_registers(struct hyp_arm_smmu_v3_device *smmu)
 {
 	u64 val;
@@ -373,6 +487,8 @@ static int smmu_init(void)
 	int ret;
 	struct hyp_arm_smmu_v3_device *smmu;
 
+	hyp_spin_lock_init(&smmu_lock);
+
 	ret = pkvm_create_mappings(kvm_hyp_arm_smmu_v3_devices,
 				   kvm_hyp_arm_smmu_v3_devices +
 				   kvm_hyp_arm_smmu_v3_nr_devices,
@@ -389,8 +505,32 @@ static int smmu_init(void)
 	return 0;
 }
 
+static int smmu_dev_set_owner(u64 smmu_base, u32 sid)
+{
+	int ret;
+	struct kvm_s2_mmu *mmu;
+	struct hyp_arm_smmu_v3_device *smmu;
+
+	mmu = kvm_host_get_stage2();
+	if (!mmu)
+		return -ENXIO;
+
+	ret = -ENODEV;
+	hyp_spin_lock(&smmu_lock);
+	for_each_smmu(smmu) {
+		if (smmu->mmio_addr == smmu_base) {
+			ret = smmu_enable_ste(smmu, sid, mmu);
+			break;
+		}
+	}
+	hyp_spin_unlock(&smmu_lock);
+	return ret;
+}
+
+
 static struct kvm_iommu_ops smmu_ops = {
 	.init				= smmu_init,
+	.dev_set_owner			= smmu_dev_set_owner,
 };
 
 int kvm_arm_smmu_v3_register(void)
-- 
2.34.1

