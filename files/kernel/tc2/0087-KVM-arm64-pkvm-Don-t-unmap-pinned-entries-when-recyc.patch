From 3ef16f409078ade105931c9dade28b298e0fdf38 Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Tue, 1 Mar 2022 19:35:53 +0000
Subject: [PATCH 87/97] KVM: arm64: pkvm: Don't unmap pinned entries when
 recycling

host_stage2_unmap_dev_all() removes any mapping outside of memory
regions in an attempt to recover some page table pages. Prevent it from
removing MMIO pages that were explicitly pinned by the host.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/include/asm/kvm_pgtable.h  | 17 +++++++++++++++++
 arch/arm64/kvm/hyp/nvhe/mem_protect.c |  8 ++++----
 arch/arm64/kvm/hyp/pgtable.c          | 22 ++++++++++++++++++++--
 3 files changed, 41 insertions(+), 6 deletions(-)

diff --git a/arch/arm64/include/asm/kvm_pgtable.h b/arch/arm64/include/asm/kvm_pgtable.h
index 377ee4c3ec47..02867737762f 100644
--- a/arch/arm64/include/asm/kvm_pgtable.h
+++ b/arch/arm64/include/asm/kvm_pgtable.h
@@ -404,6 +404,23 @@ int kvm_pgtable_stage2_annotate(struct kvm_pgtable *pgt, u64 addr, u64 size,
 int kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size,
 			     void *mc);
 
+/*
+ * kvm_pgtable_stage2_unmap_unpinned() - Remove unpinned mappings from a guest
+ *                                       stage-2 page-table
+ * @pgt:	Page-table structure initialised by kvm_pgtable_stage2_init*().
+ * @addr:	Intermediate physical address from which to remove the mapping.
+ * @size:	Size of the mapping.
+ * @mc:		Cache of pre-allocated and zeroed memory from which to allocate
+ *	        page-table pages.
+ *
+ * Just like kvm_pgtable_stage2_unmap(), except pinned entries within the range
+ * are not removed.
+ *
+ * Return: 0 on success, negative error code on failure.
+ */
+int kvm_pgtable_stage2_unmap_unpinned(struct kvm_pgtable *pgt, u64 addr,
+				      u64 size, void *mc);
+
 /**
  * kvm_pgtable_stage2_wrprotect() - Write-protect guest stage-2 address range
  *                                  without TLB invalidation.
diff --git a/arch/arm64/kvm/hyp/nvhe/mem_protect.c b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
index 085985114cad..3db27c81a31c 100644
--- a/arch/arm64/kvm/hyp/nvhe/mem_protect.c
+++ b/arch/arm64/kvm/hyp/nvhe/mem_protect.c
@@ -425,13 +425,13 @@ static int host_stage2_unmap_dev_all(void)
 	/* Unmap all non-memory regions to recycle the pages */
 	for (i = 0; i < hyp_memblock_nr; i++, addr = reg->base + reg->size) {
 		reg = &hyp_memory[i];
-		ret = host_stage2_unmap_dev_locked(addr, reg->base - addr,
-							&host_s2_pool);
+		ret = kvm_pgtable_stage2_unmap_unpinned(pgt,
+				addr, reg->base - addr, &host_s2_pool);
 		if (ret)
 			return ret;
 	}
-	return host_stage2_unmap_dev_locked(addr, BIT(pgt->ia_bits) - addr,
-						&host_s2_pool);
+	return kvm_pgtable_stage2_unmap_unpinned(pgt, addr,
+			BIT(pgt->ia_bits) - addr, &host_s2_pool);
 }
 
 struct kvm_mem_range {
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 0265dcf75c63..20899bf293d4 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -580,6 +580,8 @@ struct stage2_map_data {
 
 	/* Force mappings to page granularity */
 	bool				force_pte;
+	/* Remove pinned entries while unmapping */
+	bool				remove_pinned;
 };
 
 u64 kvm_get_vtcr(u64 mmfr0, u64 mmfr1, u32 phys_shift)
@@ -1179,6 +1181,9 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 		if (stage2_pte_cacheable(pgt, pte))
 			need_flush = !stage2_has_fwb(pgt);
 
+		if (stage2_pte_is_pinned(pgt, pte) && !data->remove_pinned)
+			return 0;
+
 		/*
 		 * If this is a block and we cannot afford lazy mapping, keep
 		 * areas around the unmap identity-mapped.
@@ -1201,13 +1206,14 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 	return 0;
 }
 
-int kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size,
-				void *mc)
+static int __kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr,
+				      u64 size, void *mc, bool remove_pinned)
 {
 	struct stage2_map_data map_data = {
 		.pgt		= pgt,
 		.memcache	= mc,
 		.force_pte	= true,
+		.remove_pinned	= remove_pinned,
 	};
 	struct kvm_pgtable_walker walker = {
 		.cb	= stage2_unmap_walker,
@@ -1218,6 +1224,18 @@ int kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return kvm_pgtable_walk(pgt, addr, size, &walker);
 }
 
+int kvm_pgtable_stage2_unmap(struct kvm_pgtable *pgt, u64 addr, u64 size,
+			     void *mc)
+{
+	return __kvm_pgtable_stage2_unmap(pgt, addr, size, mc, true);
+}
+
+int kvm_pgtable_stage2_unmap_unpinned(struct kvm_pgtable *pgt, u64 addr,
+				      u64 size, void *mc)
+{
+	return __kvm_pgtable_stage2_unmap(pgt, addr, size, mc, false);
+}
+
 struct stage2_attr_data {
 	kvm_pte_t			attr_set;
 	kvm_pte_t			attr_clr;
-- 
2.34.1

