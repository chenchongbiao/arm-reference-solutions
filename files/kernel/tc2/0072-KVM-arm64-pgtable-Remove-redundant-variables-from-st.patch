From 64b92d329ca24dd06c5dd927e3c17e93b92d8dc5 Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Fri, 25 Feb 2022 17:24:10 +0000
Subject: [PATCH 72/97] KVM: arm64: pgtable: Remove redundant variables from
 stage2_map_data

struct kvm_pgtable contains both kvm_s2_mmu and kvm_pgtable_mm_ops.
Since pgt is needed all the way down anyway (in stage2_pte_cacheable()),
pass it to stage2_map_data.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/kvm/hyp/pgtable.c | 37 ++++++++++++++++--------------------
 1 file changed, 16 insertions(+), 21 deletions(-)

diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 03bdf4f50826..062b8e251dce 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -562,11 +562,9 @@ struct stage2_map_data {
 	kvm_pte_t			*anchor;
 	kvm_pte_t			*childp;
 
-	struct kvm_s2_mmu		*mmu;
+	struct kvm_pgtable		*pgt;
 	void				*memcache;
 
-	struct kvm_pgtable_mm_ops	*mm_ops;
-
 	/* Force mappings to page granularity */
 	bool				force_pte;
 };
@@ -674,8 +672,8 @@ static bool stage2_pte_is_counted(kvm_pte_t pte)
 	return !!pte;
 }
 
-static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
-			   u32 level, struct kvm_pgtable_mm_ops *mm_ops)
+static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_pgtable *pgt, u64 addr,
+			   u32 level)
 {
 	/*
 	 * Clear the existing PTE, and perform break-before-make with
@@ -683,10 +681,10 @@ static void stage2_put_pte(kvm_pte_t *ptep, struct kvm_s2_mmu *mmu, u64 addr,
 	 */
 	if (kvm_pte_valid(*ptep)) {
 		kvm_clear_pte(ptep);
-		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, mmu, addr, level);
+		kvm_call_hyp(__kvm_tlb_flush_vmid_ipa, pgt->mmu, addr, level);
 	}
 
-	mm_ops->put_page(ptep);
+	pgt->mm_ops->put_page(ptep);
 }
 
 static bool stage2_pte_cacheable(struct kvm_pgtable *pgt, kvm_pte_t pte)
@@ -715,8 +713,8 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
 {
 	kvm_pte_t new, old = *ptep;
 	u64 granule = kvm_granule_size(level), phys = data->phys;
-	struct kvm_pgtable *pgt = data->mmu->pgt;
-	struct kvm_pgtable_mm_ops *mm_ops = data->mm_ops;
+	struct kvm_pgtable *pgt = data->pgt;
+	struct kvm_pgtable_mm_ops *mm_ops = pgt->mm_ops;
 
 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return -E2BIG;
@@ -743,7 +741,7 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,
 		if (!((old ^ new) & ~KVM_PTE_LEAF_ATTR_HI_SW))
 			goto out_set_pte;
 
-		stage2_put_pte(ptep, data->mmu, addr, level, mm_ops);
+		stage2_put_pte(ptep, pgt, addr, level);
 	}
 
 	/* Perform CMOs before installation of the guest stage-2 PTE */
@@ -773,7 +771,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 	if (!stage2_leaf_mapping_allowed(addr, end, level, data))
 		return 0;
 
-	data->childp = kvm_pte_follow(*ptep, data->mm_ops);
+	data->childp = kvm_pte_follow(*ptep, data->pgt->mm_ops);
 	kvm_clear_pte(ptep);
 
 	/*
@@ -781,7 +779,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 	 * entries below us which would otherwise need invalidating
 	 * individually.
 	 */
-	kvm_call_hyp(__kvm_tlb_flush_vmid, data->mmu);
+	kvm_call_hyp(__kvm_tlb_flush_vmid, data->pgt->mmu);
 	data->anchor = ptep;
 	return 0;
 }
@@ -789,7 +787,7 @@ static int stage2_map_walk_table_pre(u64 addr, u64 end, u32 level,
 static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 				struct stage2_map_data *data)
 {
-	struct kvm_pgtable_mm_ops *mm_ops = data->mm_ops;
+	struct kvm_pgtable_mm_ops *mm_ops = data->pgt->mm_ops;
 	kvm_pte_t *childp, pte = *ptep;
 	int ret;
 
@@ -820,7 +818,7 @@ static int stage2_map_walk_leaf(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 	 * will be mapped lazily.
 	 */
 	if (stage2_pte_is_counted(pte))
-		stage2_put_pte(ptep, data->mmu, addr, level, mm_ops);
+		stage2_put_pte(ptep, data->pgt, addr, level);
 
 	kvm_set_table_pte(ptep, childp, mm_ops);
 	mm_ops->get_page(ptep);
@@ -832,7 +830,7 @@ static int stage2_map_walk_table_post(u64 addr, u64 end, u32 level,
 				      kvm_pte_t *ptep,
 				      struct stage2_map_data *data)
 {
-	struct kvm_pgtable_mm_ops *mm_ops = data->mm_ops;
+	struct kvm_pgtable_mm_ops *mm_ops = data->pgt->mm_ops;
 	kvm_pte_t *childp;
 	int ret = 0;
 
@@ -897,9 +895,8 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	int ret;
 	struct stage2_map_data map_data = {
 		.phys		= ALIGN_DOWN(phys, PAGE_SIZE),
-		.mmu		= pgt->mmu,
+		.pgt		= pgt,
 		.memcache	= mc,
-		.mm_ops		= pgt->mm_ops,
 		.force_pte	= pgt->force_pte_cb && pgt->force_pte_cb(addr, addr + size, prot),
 	};
 	struct kvm_pgtable_walker walker = {
@@ -928,9 +925,8 @@ int kvm_pgtable_stage2_annotate(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	int ret;
 	struct stage2_map_data map_data = {
 		.phys		= KVM_PHYS_INVALID,
-		.mmu		= pgt->mmu,
+		.pgt		= pgt,
 		.memcache	= mc,
-		.mm_ops		= pgt->mm_ops,
 		.force_pte	= true,
 		.annotation	= annotation,
 	};
@@ -954,7 +950,6 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 			       void * const arg)
 {
 	struct kvm_pgtable *pgt = arg;
-	struct kvm_s2_mmu *mmu = pgt->mmu;
 	struct kvm_pgtable_mm_ops *mm_ops = pgt->mm_ops;
 	kvm_pte_t pte = *ptep, *childp = NULL;
 	bool need_flush = false;
@@ -981,7 +976,7 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 	 * block entry and rely on the remaining portions being faulted
 	 * back lazily.
 	 */
-	stage2_put_pte(ptep, mmu, addr, level, mm_ops);
+	stage2_put_pte(ptep, pgt, addr, level);
 
 	if (need_flush && mm_ops->dcache_clean_inval_poc)
 		mm_ops->dcache_clean_inval_poc(kvm_pte_follow(pte, mm_ops),
-- 
2.34.1

