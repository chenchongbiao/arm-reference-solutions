From 6325118be2e2121df6f4037a3bdcb3fba15c743f Mon Sep 17 00:00:00 2001
From: Jean-Philippe Brucker <jean-philippe@linaro.org>
Date: Mon, 28 Feb 2022 12:17:12 +0000
Subject: [PATCH 78/97] KVM: arm64: pgtable: Keep pinned idmap surrounding
 unmap()

When stage-2 is identity-mapped, don't remove mappings around the area
removed by unmap(). This requires immediately changing blocks into a
table rather than relying on lazy remapping.

Signed-off-by: Jean-Philippe Brucker <jean-philippe@linaro.org>
---
 arch/arm64/kvm/hyp/pgtable.c | 50 ++++++++++++++++++++++++++++++------
 1 file changed, 42 insertions(+), 8 deletions(-)

diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
index 6d6a34c3afd3..0265dcf75c63 100644
--- a/arch/arm64/kvm/hyp/pgtable.c
+++ b/arch/arm64/kvm/hyp/pgtable.c
@@ -1122,6 +1122,33 @@ int kvm_pgtable_stage2_annotate(struct kvm_pgtable *pgt, u64 addr, u64 size,
 	return ret;
 }
 
+static int stage2_rebuild_table_unmap(u64 start, u64 end, u32 level,
+				      struct stage2_map_data *data,
+				      kvm_pte_t block_pte, kvm_pte_t *table_pte)
+{
+	int ret;
+	kvm_pte_t *table;
+	struct kvm_pgtable *pgt = data->pgt;
+
+	/* Only block mappings */
+	if (kvm_pgt_level_is_last(level) ||
+	    WARN_ON(kvm_pte_table(block_pte, level)))
+		return 0;
+
+	if (!stage2_needs_rebuild_table(data, block_pte))
+		return 0;
+
+	table = pgt->mm_ops->zalloc_page(data->memcache);
+	if (!table)
+		return -ENOMEM;
+
+	ret = stage2_rebuild_table(start, end, level, block_pte, table, data,
+				   table_pte);
+	if (ret || !table_pte)
+		pgt->mm_ops->put_page(table);
+	return ret;
+}
+
 static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 			       enum kvm_pgtable_walk_flags flag,
 			       void * const arg)
@@ -1132,6 +1159,8 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 	struct kvm_pgtable_mm_ops *mm_ops = pgt->mm_ops;
 	kvm_pte_t pte = *ptep, *childp = NULL;
 	bool need_flush = false;
+	kvm_pte_t table_pte = 0;
+	int ret;
 
 	if (!kvm_pte_valid(pte)) {
 		if (stage2_pte_is_counted(pte)) {
@@ -1146,16 +1175,21 @@ static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,
 
 		if (mm_ops->page_count(childp) != 1)
 			return 0;
-	} else if (stage2_pte_cacheable(pgt, pte)) {
-		need_flush = !stage2_has_fwb(pgt);
+	} else {
+		if (stage2_pte_cacheable(pgt, pte))
+			need_flush = !stage2_has_fwb(pgt);
+
+		/*
+		 * If this is a block and we cannot afford lazy mapping, keep
+		 * areas around the unmap identity-mapped.
+		 */
+		ret = stage2_rebuild_table_unmap(addr, end, level, data, pte,
+						 &table_pte);
+		if (ret)
+			return ret;
 	}
 
-	/*
-	 * This is similar to the map() path in that we unmap the entire
-	 * block entry and rely on the remaining portions being faulted
-	 * back lazily.
-	 */
-	stage2_put_pte(ptep, pgt, addr, level);
+	stage2_xchg_pte(ptep, pgt, addr, level, table_pte);
 
 	if (need_flush && mm_ops->dcache_clean_inval_poc)
 		mm_ops->dcache_clean_inval_poc(kvm_pte_follow(pte, mm_ops),
-- 
2.34.1

