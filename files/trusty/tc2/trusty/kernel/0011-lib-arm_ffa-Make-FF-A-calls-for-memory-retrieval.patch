From 7f7ba0d44d6818e95c3b29b31de97677cedd37e7 Mon Sep 17 00:00:00 2001
From: Ben Horgan <ben.horgan@arm.com>
Date: Wed, 22 Sep 2021 16:17:05 +0000
Subject: [PATCH 11/22] [lib][arm_ffa] Make FF-A calls for memory retrieval

Adapted from lib/sm/shared_mem.c.
Also, stop sharing variables with lib/sm/shared_mem.c

Change-Id: If4de7ec5f36c65d664db094082125438f05898a1
Signed-off-by: Ben Horgan <ben.horgan@arm.com>
Upstream-Status: Pending [Not submitted to upstream yet]
---
 lib/arm_ffa/ffa.c                         | 421 +++++++++++++++++++++-
 lib/arm_ffa/include/lib/arm_ffa/arm_ffa.h |  13 +
 lib/arm_ffa/include/lib/arm_ffa/ffa.h     |  34 +-
 lib/sm/shared_mem.c                       | 358 +++---------------
 4 files changed, 511 insertions(+), 315 deletions(-)

diff --git a/lib/arm_ffa/ffa.c b/lib/arm_ffa/ffa.c
index b9311de..2a4482a 100644
--- a/lib/arm_ffa/ffa.c
+++ b/lib/arm_ffa/ffa.c
@@ -23,6 +23,7 @@
 
 #include <assert.h>
 #include <err.h>
+#include <inttypes.h>
 #include <string.h>
 #include <trace.h>
 #include <kernel/mutex.h>
@@ -34,12 +35,14 @@
 #include <lib/smc/smc.h>
 #include <lk/init.h>
 
+#define LOCAL_TRACE 0
+
 static bool ffa_init_is_success = false;
-uint16_t ffa_local_id;
-size_t ffa_buf_size;
-bool supports_ns_bit = false;
-void* ffa_tx;
-void* ffa_rx;
+static uint16_t ffa_local_id;
+static size_t ffa_buf_size;
+static void* ffa_tx;
+static void* ffa_rx;
+static bool supports_ns_bit = false;
 
 static mutex_t ffa_rxtx_buffer_lock = MUTEX_INITIAL_VALUE(ffa_rxtx_buffer_lock);
 
@@ -133,6 +136,89 @@ static status_t ffa_call_features(ulong id, bool request_ns_bit,
     }
 }
 
+/* Call with ffa_rxtx_buffer_lock acquired and the ffa_tx buffer already
+   populated with struct ffa_mtd. Transmit in a single fragment. */
+static status_t ffa_call_mem_retrieve_req(uint32_t *total_len,
+                                          uint32_t *fragment_len) {
+    struct smc_ret8 smc_ret;
+    struct ffa_mtd* req = ffa_tx;
+    size_t len;
+
+    DEBUG_ASSERT(is_mutex_held(&ffa_rxtx_buffer_lock));
+
+    len = offsetof(struct ffa_mtd, emad[0]) +
+      req->emad_count * sizeof(struct ffa_emad);
+
+    smc_ret = smc8(SMC_FC_FFA_MEM_RETRIEVE_REQ, len, len, 0, 0, 0, 0, 0);
+
+    switch (smc_ret.r0) {
+    case SMC_FC_FFA_MEM_RETRIEVE_RESP:
+        if (total_len) {
+            *total_len = (uint32_t) smc_ret.r1;
+        }
+        if (fragment_len) {
+            *fragment_len = (uint32_t) smc_ret.r2;
+        }
+        return NO_ERROR;
+    case SMC_FC_FFA_ERROR:
+        switch ((int)smc_ret.r2) {
+        case FFA_ERROR_NOT_SUPPORTED:
+            return ERR_NOT_SUPPORTED;
+        case FFA_ERROR_INVALID_PARAMETERS:
+            return ERR_INVALID_ARGS;
+        case FFA_ERROR_NO_MEMORY:
+            return ERR_NO_MEMORY;
+        case FFA_ERROR_DENIED:
+            return ERR_BAD_STATE;
+        case FFA_ERROR_ABORTED:
+            return ERR_CANCELLED;
+        default:
+            return ERR_NOT_VALID;
+        }
+    default:
+        return ERR_NOT_VALID;
+    }
+}
+
+static status_t ffa_call_mem_frag_rx(uint64_t handle, uint32_t offset,
+                                     uint32_t *fragment_len) {
+    struct smc_ret8 smc_ret;
+
+    DEBUG_ASSERT(is_mutex_held(&ffa_rxtx_buffer_lock));
+
+    smc_ret = smc8(SMC_FC_FFA_MEM_FRAG_RX,
+                   (uint32_t) handle, handle >> 32, offset, 0, 0, 0, 0);
+
+    switch (smc_ret.r0) {
+    case SMC_FC_FFA_MEM_FRAG_TX:
+        {
+            uint64_t handle_out = smc_ret.r1 + (smc_ret.r2 << 32);
+            if (handle != handle_out) {
+                TRACEF("Handle for response doesn't match the request, %" PRId64 " != %" PRId64,
+                       handle, handle_out);
+                return ERR_NOT_VALID;
+            }
+            *fragment_len = smc_ret.r3;
+            return NO_ERROR;
+        }
+    case SMC_FC_FFA_ERROR:
+        switch ((int)smc_ret.r2) {
+        case FFA_ERROR_NOT_SUPPORTED:
+            return ERR_NOT_SUPPORTED;
+        case FFA_ERROR_INVALID_PARAMETERS:
+            return ERR_INVALID_ARGS;
+        case FFA_ERROR_ABORTED:
+            return ERR_CANCELLED;
+        default:
+            TRACEF("Unexpected error %d\n", (int)smc_ret.r2);
+            return ERR_NOT_VALID;
+        }
+    default:
+        TRACEF("Unexpected function id returned 0x%08lx\n", smc_ret.r0);
+        return ERR_NOT_VALID;
+    }
+}
+
 static status_t ffa_call_mem_relinquish(uint64_t handle, uint32_t flags,
                                         uint32_t endpoint_count,
                                         const ffa_endpoint_id16_t *endpoints) {
@@ -215,6 +301,31 @@ static status_t ffa_call_rxtx_map(paddr_t tx_paddr, paddr_t rx_paddr,
     }
 }
 
+static status_t ffa_call_rx_release(void) {
+    struct smc_ret8 smc_ret;
+
+    DEBUG_ASSERT(is_mutex_held(&ffa_rxtx_buffer_lock));
+
+    smc_ret = smc8(SMC_FC_FFA_RX_RELEASE, 0, 0, 0, 0, 0, 0, 0);
+    switch (smc_ret.r0) {
+    case SMC_FC_FFA_SUCCESS:
+    case SMC_FC64_FFA_SUCCESS:
+        return NO_ERROR;
+
+    case SMC_FC_FFA_ERROR:
+        switch ((int)smc_ret.r2) {
+        case FFA_ERROR_NOT_SUPPORTED:
+            return ERR_NOT_SUPPORTED;
+        case FFA_ERROR_DENIED:
+            return ERR_BAD_STATE;
+        default:
+            return ERR_NOT_VALID;
+        }
+    default:
+        return ERR_NOT_VALID;
+    }
+}
+
 static bool ffa_rxtx_map_is_implemented(size_t *buf_size_log2) {
     ffa_features2_t features2;
     bool is_implemented = false;
@@ -278,6 +389,306 @@ bool ffa_mem_retrieve_req_is_implemented(bool request_ns_bit,
     return true;
 }
 
+/* Helper function to set up the tx buffer with standard values
+   before calling FFA_MEM_RETRIEVE_REQ. */
+static void ffa_populate_receive_req_tx_buffer(uint16_t sender_id,
+                                               uint64_t handle,
+                                               uint64_t tag) {
+    struct ffa_mtd* req = ffa_tx;
+    DEBUG_ASSERT(is_mutex_held(&ffa_rxtx_buffer_lock));
+
+    req->sender_id = sender_id;
+
+    /* Accept any memory region attributes. */
+    req->memory_region_attributes = 0;
+
+    req->reserved_3 = 0;
+    req->flags = 0;
+    req->handle = handle;
+
+    /* We must use the same tag as the one used by the sender to retrieve. */
+    req->tag = tag;
+    req->reserved_24_27 = 0;
+
+    /*
+     * We only support retrieving memory for ourselves for now.
+     * TODO: Also support stream endpoints. Possibly more than one.
+     */
+    req->emad_count = 1;
+    req->emad[0].mapd.endpoint_id = ffa_local_id;
+
+    /* Accept any memory access permissions. */
+    req->emad[0].mapd.memory_access_permissions = 0;
+    req->emad[0].mapd.flags = 0;
+
+    /*
+     * Set composite memory region descriptor offset to 0 to indicate that the
+     * relayer should allocate the address ranges. Other values will not work
+     * for relayers that use identity maps (e.g. EL3).
+     */
+    req->emad[0].comp_mrd_offset = 0;
+    req->emad[0].reserved_8_15 = 0;
+}
+
+/* *desc_buffer is malloc'd and on success passes responsibility to free to
+   the caller. Populate the tx buffer before calling. */
+static status_t ffa_mem_retrieve(uint16_t sender_id,
+                                 uint64_t handle,
+                                 uint32_t *len,
+                                 uint32_t *fragment_len) {
+    status_t res = NO_ERROR;
+
+    DEBUG_ASSERT(is_mutex_held(&ffa_rxtx_buffer_lock));
+    DEBUG_ASSERT(len);
+
+    res = ffa_call_mem_retrieve_req(len, fragment_len);
+    LTRACEF("total_len: %u, fragment_len: %u\n", *len, *fragment_len);
+    if (res != NO_ERROR) {
+        TRACEF("FF-A memory retrieve request failed, err = %d\n", res);
+        return res;
+    }
+    if (*fragment_len > *len) {
+        TRACEF("Fragment length larger than total length %u > %u\n",
+               *fragment_len, *len);
+        return ERR_IO;
+    }
+
+    /* Check that the first fragment fits in our buffer */
+    if (*fragment_len > ffa_buf_size) {
+        TRACEF("Fragment length %u larger than buffer size\n", *fragment_len);
+        return ERR_IO;
+    }
+
+    return NO_ERROR;
+}
+
+status_t ffa_mem_address_range_get(struct ffa_mem_frag_info *frag_info,
+                                   size_t index,
+                                   paddr_t *addr,
+                                   size_t *size) {
+    uint32_t page_count;
+    size_t frag_idx;
+
+    DEBUG_ASSERT(frag_info);
+
+    if (index < frag_info->start_index ||
+        index >= frag_info->start_index + frag_info->count) {
+        return ERR_OUT_OF_RANGE;
+    }
+
+    frag_idx = index - frag_info->start_index;
+
+    page_count = frag_info->address_ranges[frag_idx].page_count;
+    LTRACEF("address 0x%016llx, page_count 0x%x\n",
+           frag_info->address_ranges[frag_idx].address,
+           frag_info->address_ranges[frag_idx].page_count);
+    STATIC_ASSERT(sizeof(page_count) < (SIZE_MAX / FFA_PAGE_SIZE));
+    if (page_count < 1) {
+        TRACEF("bad page count 0x%x at %zd\n", page_count, index);
+        return ERR_IO;
+    }
+
+    if (addr) {
+        *addr = (paddr_t) frag_info->address_ranges[frag_idx].address;
+    }
+    if (size) {
+        *size = page_count * FFA_PAGE_SIZE;
+    }
+
+    return NO_ERROR;
+}
+
+status_t ffa_mem_retrieve_start(uint16_t sender_id,
+                                uint64_t handle,
+                                uint64_t tag,
+                                uint32_t *address_range_count,
+                                uint *arch_mmu_flags,
+                                struct ffa_mem_frag_info* frag_info) {
+    status_t res;
+    struct ffa_mtd* mtd;
+    struct ffa_emad* emad;
+    struct ffa_comp_mrd* comp_mrd;
+    uint32_t computed_len;
+    uint32_t header_size;
+
+    uint32_t total_len;
+    uint32_t fragment_len;
+
+    DEBUG_ASSERT(frag_info);
+
+    mutex_acquire(&ffa_rxtx_buffer_lock);
+    ffa_populate_receive_req_tx_buffer(sender_id, handle, tag);
+    res = ffa_mem_retrieve(sender_id, handle, &total_len, &fragment_len);
+
+    if (res != NO_ERROR) {
+        TRACEF("FF-A memory retrieve failed err=%d\n", res);
+        return res;
+    }
+
+    if (fragment_len <
+        offsetof(struct ffa_mtd, emad) + sizeof(struct ffa_emad)) {
+        TRACEF("Fragment too short for memory transaction descriptor\n");
+        return ERR_IO;
+    }
+
+    mtd = ffa_rx;
+    emad = mtd->emad;
+
+    /*
+     * We don't retrieve the memory on behalf of anyone else, so we only
+     * expect one receiver address range descriptor.
+     */
+    if (mtd->emad_count != 1) {
+        TRACEF("unexpected response count %d != 1\n", mtd->emad_count);
+        return ERR_NOT_IMPLEMENTED;
+    }
+
+    LTRACEF("comp_mrd_offset: %u\n", emad->comp_mrd_offset);
+    if (emad->comp_mrd_offset + sizeof(*comp_mrd) > fragment_len) {
+        TRACEF("Fragment length %u too short for comp_mrd_offset %u\n",
+               fragment_len, emad->comp_mrd_offset);
+        return ERR_IO;
+    }
+
+    comp_mrd = ffa_rx + emad->comp_mrd_offset;
+
+    *address_range_count = comp_mrd->address_range_count;
+    frag_info->address_ranges = comp_mrd->address_range_array;
+    LTRACEF("address_range_count: %u\n", *address_range_count);
+
+    computed_len = emad->comp_mrd_offset +
+      offsetof(struct ffa_comp_mrd, address_range_array) +
+      sizeof (struct ffa_cons_mrd) * comp_mrd->address_range_count;
+    if (total_len != computed_len) {
+        TRACEF("Reported length %u != computed length %u\n",
+               total_len, computed_len);
+        return ERR_IO;
+    }
+
+    header_size = emad->comp_mrd_offset +
+      offsetof(struct ffa_comp_mrd, address_range_array);
+    frag_info->count = (fragment_len - header_size) /
+      sizeof(struct ffa_cons_mrd);
+    LTRACEF("Descriptors in fragment %u\n", frag_info->count);
+
+    if (frag_info->count * sizeof(struct ffa_cons_mrd) + header_size !=
+        fragment_len) {
+        TRACEF("fragment length %u, contains partial descriptor\n",
+               fragment_len);
+        return ERR_IO;
+    }
+
+    frag_info->received_len = fragment_len;
+    frag_info->start_index = 0;
+
+    /* Set the arch_mmu_flags */
+    *arch_mmu_flags = 0;
+
+    switch (mtd->flags & FFA_MTD_FLAG_TYPE_MASK) {
+    case FFA_MTD_FLAG_TYPE_SHARE_MEMORY:
+        /*
+         * If memory is shared, assume it is not safe to execute out of. This
+         * specifically indicates that another party may have access to the
+         * memory.
+         */
+        *arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
+        break;
+    case FFA_MTD_FLAG_TYPE_LEND_MEMORY:
+        break;
+    case FFA_MTD_FLAG_TYPE_DONATE_MEMORY:
+        TRACEF("Unexpected donate memory transaction type is not supported\n");
+        return ERR_NOT_IMPLEMENTED;
+    default:
+        TRACEF("Unknown memory transaction type\n");
+        return ERR_NOT_VALID;
+    }
+
+    switch (mtd->memory_region_attributes & ~FFA_MEM_ATTR_NONSECURE) {
+    case FFA_MEM_ATTR_DEVICE_NGNRE:
+        *arch_mmu_flags |= ARCH_MMU_FLAG_UNCACHED_DEVICE;
+        break;
+    case FFA_MEM_ATTR_NORMAL_MEMORY_UNCACHED:
+        *arch_mmu_flags |= ARCH_MMU_FLAG_UNCACHED;
+        break;
+    case (FFA_MEM_ATTR_NORMAL_MEMORY_CACHED_WB | FFA_MEM_ATTR_INNER_SHAREABLE):
+        *arch_mmu_flags |= ARCH_MMU_FLAG_CACHED;
+        break;
+    default:
+        TRACEF("Invalid memory attributes, 0x%x\n",
+               mtd->memory_region_attributes);
+        return ERR_NOT_VALID;
+    }
+
+    if (!(emad->mapd.memory_access_permissions & FFA_MEM_PERM_RW)) {
+        *arch_mmu_flags |= ARCH_MMU_FLAG_PERM_RO;
+    }
+    if (emad->mapd.memory_access_permissions & FFA_MEM_PERM_NX) {
+        /*
+         * Don't allow executable mappings if the stage 2 page tables don't
+         * allow it. The hardware allows the stage 2 NX bit to only apply to
+         * EL1, not EL0, but neither FF-A nor LK can currently express this, so
+         * disallow both if FFA_MEM_PERM_NX is set.
+         */
+        *arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
+    }
+
+    if (!supports_ns_bit ||
+        (mtd->memory_region_attributes & FFA_MEM_ATTR_NONSECURE)) {
+        *arch_mmu_flags |= ARCH_MMU_FLAG_NS;
+        /*
+         * Regardless of origin, we don't want to execute out of NS memory.
+         */
+        *arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
+    }
+
+    return res;
+}
+
+/* This assumes that the fragment is completely composed of memory
+   region descriptors (struct ffa_cons_mrd) */
+status_t ffa_mem_retrieve_next_frag(uint64_t handle,
+                                    struct ffa_mem_frag_info *frag_info) {
+    status_t res;
+    uint32_t fragment_len;
+
+    mutex_acquire(&ffa_rxtx_buffer_lock);
+
+    res = ffa_call_mem_frag_rx(handle, frag_info->received_len, &fragment_len);
+
+    if (res != NO_ERROR) {
+        TRACEF("Failed to get memory retrieve fragment, err = %d\n", res);
+        return res;
+    }
+
+    frag_info->received_len += fragment_len;
+    frag_info->start_index += frag_info->count;
+
+    frag_info->count = fragment_len / sizeof(struct ffa_cons_mrd);
+    if (frag_info->count * sizeof(struct ffa_cons_mrd) != fragment_len) {
+        TRACEF("fragment length %u, contains partial descriptor\n",
+               fragment_len);
+        return ERR_IO;
+    }
+
+    frag_info->address_ranges = ffa_rx;
+
+    return NO_ERROR;
+}
+
+status_t ffa_rx_release(void) {
+    status_t res;
+
+    res = ffa_call_rx_release();
+    mutex_release(&ffa_rxtx_buffer_lock);
+
+    if (res != NO_ERROR && res != ERR_NOT_SUPPORTED) {
+        TRACEF("Failed to release rx buffer, err = %d\n", res);
+        return res;
+    } else {
+        return NO_ERROR;
+    }
+}
+
 status_t ffa_mem_relinquish(uint64_t handle) {
     status_t res;
 
diff --git a/lib/arm_ffa/include/lib/arm_ffa/arm_ffa.h b/lib/arm_ffa/include/lib/arm_ffa/arm_ffa.h
index 6deb862..3ad41ea 100644
--- a/lib/arm_ffa/include/lib/arm_ffa/arm_ffa.h
+++ b/lib/arm_ffa/include/lib/arm_ffa/arm_ffa.h
@@ -465,6 +465,19 @@ enum ffa_error {
  */
 #define SMC_FC_FFA_FEATURES SMC_FASTCALL_NR_SHARED_MEMORY(0x64)
 
+/**
+ * SMC_FC_FFA_RX_RELEASE - SMC opcode to Relinquish ownership of a RX buffer
+ *
+ * Return:
+ * * w0:     &SMC_FC_FFA_SUCCESS
+ *
+ * or
+ *
+ * * w0:     &SMC_FC_FFA_ERROR
+ * * w2:     %FFA_ERROR_DENIED Caller did not have ownership of the RX buffer.
+ *           %FFA_ERROR_NOT_SUPPORTED if operation not supported
+ */
+#define SMC_FC_FFA_RX_RELEASE SMC_FASTCALL_NR_SHARED_MEMORY(0x65)
 /**
  * SMC_FC_FFA_RXTX_MAP - 32 bit SMC opcode to map message buffers
  *
diff --git a/lib/arm_ffa/include/lib/arm_ffa/ffa.h b/lib/arm_ffa/include/lib/arm_ffa/ffa.h
index 7e9ec8b..7f651ec 100644
--- a/lib/arm_ffa/include/lib/arm_ffa/ffa.h
+++ b/lib/arm_ffa/include/lib/arm_ffa/ffa.h
@@ -28,13 +28,31 @@
    returned. */
 bool ffa_is_init(void);
 
-/* TODO: Temporary share variables with lib/sm/shared_mem.c while
-   implementation is being moved to lib/arm_ffa. */
-extern uint16_t ffa_local_id;
-extern size_t ffa_buf_size;
-extern bool supports_ns_bit;
-extern void* ffa_tx;
-extern void* ffa_rx;
-
 /* Relinquish trusty's access to a memory region. */
 status_t ffa_mem_relinquish(uint64_t handle);
+
+struct ffa_cons_mrd;
+struct ffa_mem_frag_info {
+    uint32_t received_len;
+    size_t start_index;
+    uint32_t count;
+    /* Pointer into buffer */
+    struct ffa_cons_mrd *address_ranges;
+};
+status_t ffa_mem_address_range_get(struct ffa_mem_frag_info *buffer,
+                                   size_t index,
+                                   paddr_t *addr,
+                                   size_t *size);
+/* Retrieve a memory region from the SPMC/hypervisor for access from trusty. */
+/* benhor01: TODO: Comments about usage and locks held. */
+/* benhor01: TODO: Comments about assumptions. Only receiving for self. */
+status_t ffa_mem_retrieve_start(uint16_t sender_id,
+                                uint64_t handle,
+                                uint64_t tag,
+                                uint32_t *address_range_count,
+                                uint *arch_mmu_flags,
+                                struct ffa_mem_frag_info* frag_info);
+status_t ffa_mem_retrieve_next_frag(uint64_t handle,
+                                    struct ffa_mem_frag_info *frag_info);
+status_t ffa_rx_release(void);
+
diff --git a/lib/sm/shared_mem.c b/lib/sm/shared_mem.c
index d054d22..e957a20 100644
--- a/lib/sm/shared_mem.c
+++ b/lib/sm/shared_mem.c
@@ -42,8 +42,6 @@ struct sm_mem_obj {
     struct ext_mem_obj ext_mem_obj;
 };
 
-static mutex_t sm_mem_ffa_lock = MUTEX_INITIAL_VALUE(sm_mem_ffa_lock);
-
 static void sm_mem_obj_compat_destroy(struct vmm_obj* vmm_obj) {
     struct ext_mem_obj* obj = containerof(vmm_obj, struct ext_mem_obj, vmm_obj);
     free(obj);
@@ -124,12 +122,10 @@ static void sm_mem_obj_destroy(struct vmm_obj* vmm_obj) {
 
     DEBUG_ASSERT(obj);
 
-    mutex_acquire(&sm_mem_ffa_lock);
     ret = ffa_mem_relinquish(obj->ext_mem_obj.id);
     if (ret != NO_ERROR) {
         TRACEF("Failed to relinquish the shared memory\n");
     }
-    mutex_release(&sm_mem_ffa_lock);
 
     free(obj);
 }
@@ -169,313 +165,73 @@ static struct sm_mem_obj* sm_mem_alloc_obj(uint16_t sender_id,
     return obj;
 }
 
-/**
- * ffa_mem_retrieve_req - Call SPM/Hypervisor to retrieve memory region.
- * @sender_id:  FF-A vm id of sender.
- * @handle:     FF-A allocated handle.
- *
- * Helper function to start retrieval. Does not process result.
- *
- * Return: &struct smc_ret8.
- */
-static struct smc_ret8 ffa_mem_retrieve_req(uint16_t sender_id,
-                                            uint64_t handle,
-                                            uint64_t tag) {
-    struct ffa_mtd* req = ffa_tx;
-
-    DEBUG_ASSERT(is_mutex_held(&sm_mem_ffa_lock));
-
-    req->sender_id = sender_id;
-
-    /* Accept any memory region attributes. */
-    req->memory_region_attributes = 0;
-
-    req->reserved_3 = 0;
-    req->flags = 0;
-    req->handle = handle;
-
-    /* We must use the same tag as the one used by the sender to retrieve. */
-    req->tag = tag;
-    req->reserved_24_27 = 0;
 
-    /*
-     * We only support retrieving memory for ourselves for now.
-     * TODO: Also support stream endpoints. Possibly more than one.
-     */
-    req->emad_count = 1;
-    req->emad[0].mapd.endpoint_id = ffa_local_id;
-
-    /* Accept any memory access permissions. */
-    req->emad[0].mapd.memory_access_permissions = 0;
-    req->emad[0].mapd.flags = 0;
-
-    /*
-     * Set composite memory region descriptor offset to 0 to indicate that the
-     * relayer should allocate the address ranges. Other values will not work
-     * for relayers that use identity maps (e.g. EL3).
-     */
-    req->emad[0].comp_mrd_offset = 0;
-    req->emad[0].reserved_8_15 = 0;
-
-    size_t len = offsetof(struct ffa_mtd, emad[1]);
-
-    /* Start FFA_MEM_RETRIEVE_REQ. */
-    return smc8(SMC_FC_FFA_MEM_RETRIEVE_REQ, len, len, 0, 0, 0, 0, 0);
-}
-
-/**
- * ffa_mem_retrieve - Call SPM/Hypervisor to retrieve memory region.
- * @sender_id:  FF-A vm id of sender.
- * @handle:     FF-A allocated handle.
- * @objp:       Pointer to return object in.
- * @obj_ref:    Reference to *@objp.
- *
- * Return: 0 on success, lk error code on failure.
+/* sm_mem_get_vmm_obj - Implementation of ext_mem_get_vmm_obj using
+ *                      FF-A shared memory.
  */
-static int ffa_mem_retrieve(uint16_t sender_id,
-                            uint64_t handle,
-                            uint64_t tag,
-                            struct vmm_obj** objp,
-                            struct obj_ref* obj_ref) {
-    struct smc_ret8 smc_ret;
-    struct ffa_mtd* resp = ffa_rx;
-    struct ffa_emad* emad = resp->emad;
-    struct sm_mem_obj* obj;
-    struct obj_ref tmp_obj_ref = OBJ_REF_INITIAL_VALUE(tmp_obj_ref);
+static status_t sm_mem_get_vmm_obj(ext_mem_client_id_t client_id,
+                                   ext_mem_obj_id_t mem_obj_id,
+                                   uint64_t tag,
+                                   size_t size,
+                                   struct vmm_obj** objp,
+                                   struct obj_ref* obj_ref) {
     int ret;
+    struct ffa_mem_frag_info frag_info;
+    uint32_t address_range_count;
     uint arch_mmu_flags;
-    struct ffa_comp_mrd* comp_mrd;
-
-    DEBUG_ASSERT(is_mutex_held(&sm_mem_ffa_lock));
-    DEBUG_ASSERT(objp);
-    DEBUG_ASSERT(obj_ref);
-
-    if (!ffa_tx) {
-        TRACEF("no FF-A buffer\n");
-        return ERR_NOT_READY;
-    }
-
-    smc_ret = ffa_mem_retrieve_req(sender_id, handle, tag);
-    if ((uint32_t)smc_ret.r0 != SMC_FC_FFA_MEM_RETRIEVE_RESP) {
-        TRACEF("bad reply: 0x%lx 0x%lx 0x%lx\n", smc_ret.r0, smc_ret.r1,
-               smc_ret.r2);
-        return ERR_IO;
-    }
-    size_t total_len = (uint32_t)smc_ret.r1;
-    size_t fragment_len = (uint32_t)smc_ret.r2;
-
-    /*
-     * We don't retrieve the memory on behalf of anyone else, so we only
-     * expect one receiver address range descriptor.
-     */
-    if (resp->emad_count != 1) {
-        TRACEF("unexpected response count %d != 1\n", resp->emad_count);
-    }
-
-    switch (resp->flags & FFA_MTD_FLAG_TYPE_MASK) {
-    case FFA_MTD_FLAG_TYPE_SHARE_MEMORY:
-    case FFA_MTD_FLAG_TYPE_LEND_MEMORY:
-        break;
-    default:
-        /* Donate or an unknown sharing type */
-        TRACEF("Unknown transfer kind: 0x%x\n",
-               resp->flags & FFA_MTD_FLAG_TYPE_MASK);
-        return ERR_IO;
-    }
-
-    /* Check that the first fragment contains the entire header. */
-    size_t header_size = offsetof(struct ffa_mtd, emad[1]);
-    if (fragment_len < header_size) {
-        TRACEF("fragment length %zd too short\n", fragment_len);
-        return ERR_IO;
-    }
-
-    /* Check that the first fragment fits in our buffer */
-    if (fragment_len > ffa_buf_size) {
-        TRACEF("fragment length %zd larger than buffer size\n", fragment_len);
-        return ERR_IO;
-    }
-
-    size_t comp_mrd_offset = emad->comp_mrd_offset;
-
-    /*
-     * We have already checked that fragment_len is larger than *resp. Since
-     * *comp_mrd is smaller than that (verified here), the fragment_len -
-     * sizeof(*comp_mrd) subtraction below will never underflow.
-     */
-    STATIC_ASSERT(sizeof(*resp) >= sizeof(*comp_mrd));
-
-    if (comp_mrd_offset > fragment_len - sizeof(*comp_mrd)) {
-        TRACEF("fragment length %zd too short for comp_mrd_offset %zd\n",
-               fragment_len, comp_mrd_offset);
-        return ERR_IO;
-    }
-    comp_mrd = (void*)resp + comp_mrd_offset;
-
-    /*
-     * Set arch_mmu_flags based on mem_attr returned.
-     */
-    switch (resp->memory_region_attributes & ~FFA_MEM_ATTR_NONSECURE) {
-    case FFA_MEM_ATTR_DEVICE_NGNRE:
-        arch_mmu_flags = ARCH_MMU_FLAG_UNCACHED_DEVICE;
-        break;
-    case FFA_MEM_ATTR_NORMAL_MEMORY_UNCACHED:
-        arch_mmu_flags = ARCH_MMU_FLAG_UNCACHED;
-        break;
-    case (FFA_MEM_ATTR_NORMAL_MEMORY_CACHED_WB | FFA_MEM_ATTR_INNER_SHAREABLE):
-        arch_mmu_flags = ARCH_MMU_FLAG_CACHED;
-        break;
-    default:
-        TRACEF("unsupported memory attributes, 0x%x\n",
-               resp->memory_region_attributes);
-        return ERR_NOT_SUPPORTED;
-    }
-
-    if (!supports_ns_bit || (resp->memory_region_attributes & FFA_MEM_ATTR_NONSECURE)) {
-        arch_mmu_flags |= ARCH_MMU_FLAG_NS;
-    } else {
-        LTRACEF("secure memory path triggered\n");
-    }
-
-    if (!(emad->mapd.memory_access_permissions & FFA_MEM_PERM_RW)) {
-        arch_mmu_flags |= ARCH_MMU_FLAG_PERM_RO;
-    }
-    if (emad->mapd.memory_access_permissions & FFA_MEM_PERM_NX) {
-        /*
-         * Don't allow executable mappings if the stage 2 page tables don't
-         * allow it. The hardware allows the stage 2 NX bit to only apply to
-         * EL1, not EL0, but neither FF-A nor LK can currently express this, so
-         * disallow both if FFA_MEM_PERM_NX is set.
-         */
-        arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
-    }
-
-    if ((resp->flags & FFA_MTD_FLAG_TYPE_MASK) ==
-        FFA_MTD_FLAG_TYPE_SHARE_MEMORY) {
-        /*
-         * If memory is shared, assume it is not safe to execute out of. This
-         * specifically indicates that another party may have access to the
-         * memory.
-         */
-        arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
-    }
-
-    /*
-     * Regardless of origin, we don't want to execute out of NS memory.
-     */
-    if (arch_mmu_flags & ARCH_MMU_FLAG_NS) {
-        arch_mmu_flags |= ARCH_MMU_FLAG_PERM_NO_EXECUTE;
-    }
-
-    /*
-     * Check that the overall length of the message matches the expected length
-     * for the number of entries specified in the header.
-     */
-    uint32_t address_range_descriptor_count = comp_mrd->address_range_count;
-    size_t expected_len =
-            comp_mrd_offset +
-            offsetof(struct ffa_comp_mrd,
-                     address_range_array[address_range_descriptor_count]);
-    if (total_len != expected_len) {
-        TRACEF("length mismatch smc %zd != computed %zd for count %d\n",
-               total_len, expected_len, address_range_descriptor_count);
-        return ERR_IO;
-    }
-
-    header_size = comp_mrd_offset + sizeof(*comp_mrd);
-
-    struct ffa_cons_mrd* desc = comp_mrd->address_range_array;
+    struct sm_mem_obj* obj;
+    struct obj_ref tmp_obj_ref = OBJ_REF_INITIAL_VALUE(tmp_obj_ref);
 
-    /*
-     * Compute full descriptor count and size of partial descriptor in first
-     * fragment.
-     */
-    size_t desc_count = (fragment_len - header_size) / sizeof(*desc);
-    if (desc_count * sizeof(*desc) + header_size != fragment_len) {
-        TRACEF("fragment length %zd, contains partial descriptor\n",
-               fragment_len);
-        return ERR_IO;
-    }
+    ret = ffa_mem_retrieve_start((uint16_t)client_id, mem_obj_id, tag,
+                                 &address_range_count, &arch_mmu_flags,
+                                 &frag_info);
 
-    /* The first fragment should not be larger than the whole message */
-    if (desc_count > address_range_descriptor_count) {
-        TRACEF("bad fragment length %zd > %zd\n", fragment_len, total_len);
-        return ERR_IO;
+    if (ret != NO_ERROR) {
+        TRACEF("Failed to get FF-A memory buffer, err=%d\n", ret);
+        goto err_mem_get_access;
     }
-
-    LTRACEF("handle %lld, desc count %d\n", handle,
-            address_range_descriptor_count);
-
-    /* Allocate a new shared memory object. */
-    obj = sm_mem_alloc_obj(sender_id, handle, tag,
-                           address_range_descriptor_count, arch_mmu_flags,
-                           &tmp_obj_ref);
+    obj = sm_mem_alloc_obj(client_id, mem_obj_id, tag, address_range_count,
+                           arch_mmu_flags, &tmp_obj_ref);
     if (!obj) {
-        return ERR_NO_MEMORY;
+        TRACEF("Failed to allocate a shared memory object\n");
+        ret = ERR_NO_MEMORY;
+        goto err_mem_alloc_obj;
     }
 
-    for (uint ri = 0, di = 0; ri < address_range_descriptor_count; ri++, di++) {
-        if (di >= desc_count) {
-            mutex_release(&sm_mem_ffa_lock);
-            /* Drop lock to allow interleaving large object retrieval */
-            mutex_acquire(&sm_mem_ffa_lock);
-            /*
-             * All descriptors in this fragment has been consumed.
-             * Fetch next fragment from the SPM/Hypervisor.
-             */
-            smc_ret = smc8(SMC_FC_FFA_MEM_FRAG_RX, (uint32_t)handle,
-                           handle >> 32, fragment_len, 0, 0, 0, 0);
-            if ((uint32_t)smc_ret.r0 != SMC_FC_FFA_MEM_FRAG_TX) {
-                TRACEF("bad reply: 0x%lx 0x%lx 0x%lx\n", smc_ret.r0, smc_ret.r1,
-                       smc_ret.r2);
-                ret = ERR_IO;
-                goto err_mem_frag_rx;
-            }
-            fragment_len += (uint32_t)smc_ret.r3;
-
-            desc = ffa_rx;
-            di = 0;
-
-            /*
-             * Compute descriptor count in this fragment.
-             */
-            desc_count = ((uint32_t)smc_ret.r3) / sizeof(*desc);
-            if ((uint32_t)smc_ret.r3 != desc_count * sizeof(*desc)) {
-                TRACEF("fragment length %ld, contains partial descriptor\n",
-                       smc_ret.r3);
-                ret = ERR_IO;
-                goto err_bad_data;
+    for (uint32_t i=0; i<address_range_count; i++) {
+        if (frag_info.start_index + frag_info.count <= i) {
+            ffa_rx_release();
+            ret = ffa_mem_retrieve_next_frag(mem_obj_id, &frag_info);
+            if (ret != NO_ERROR) {
+                TRACEF("Failed to get next fragment, err=%d\n", ret);
+                goto err_mem_next_frag;
             }
         }
-
-        /* Copy one descriptor into object */
-        obj->ext_mem_obj.page_runs[ri].paddr = desc[di].address;
-        if (desc[di].page_count < 1 ||
-            ((size_t)desc[di].page_count > (SIZE_MAX / FFA_PAGE_SIZE))) {
-            TRACEF("bad page count 0x%x at %d/%d %d/%zd\n", desc[di].page_count,
-                   ri, address_range_descriptor_count, di, desc_count);
-            ret = ERR_IO;
-            goto err_bad_data;
+        ret = ffa_mem_address_range_get(&frag_info, i,
+                                        &obj->ext_mem_obj.page_runs[i].paddr,
+                                        &obj->ext_mem_obj.page_runs[i].size);
+        if (ret != NO_ERROR) {
+            TRACEF("Failed to get address range, err=%d\n", ret);
+            goto err_mem_address_range;
         }
-        obj->ext_mem_obj.page_runs[ri].size =
-                (size_t)desc[di].page_count * FFA_PAGE_SIZE;
-        LTRACEF("added ns memory at 0x%lx, size %zd, %d/%d %d/%zd\n",
-                obj->ext_mem_obj.page_runs[ri].paddr,
-                obj->ext_mem_obj.page_runs[ri].size, ri,
-                address_range_descriptor_count, di, desc_count);
     }
 
     /* No lock needed as the object is not yet visible to anyone else */
     obj_ref_transfer(obj_ref, &tmp_obj_ref);
     *objp = &obj->ext_mem_obj.vmm_obj;
 
+    ffa_rx_release();
+
     return 0;
 
-err_mem_frag_rx:
-err_bad_data:
+err_mem_address_range:
+err_mem_next_frag:
     DEBUG_ASSERT(obj_ref_active(&tmp_obj_ref));
     vmm_obj_del_ref(&obj->ext_mem_obj.vmm_obj, &tmp_obj_ref);
 
+err_mem_alloc_obj:
+err_mem_get_access:
+    ffa_rx_release();
     return ret;
 }
 
@@ -483,6 +239,7 @@ err_bad_data:
  * ext_mem_get_vmm_obj - Lookup or create shared memory object.
  * @client_id:  Id of external entity where the memory originated.
  * @mem_obj_id: Id of shared memory opbject to lookup and return.
+ * @tag:        Value to identify the transaction
  * @size:       Size hint for object.
  * @objp:       Pointer to return object in.
  * @obj_ref:    Reference to *@objp.
@@ -496,22 +253,19 @@ status_t ext_mem_get_vmm_obj(ext_mem_client_id_t client_id,
                              size_t size,
                              struct vmm_obj** objp,
                              struct obj_ref* obj_ref) {
-    int ret;
-
-    if (client_id == 0 && tag == 0 &&
-        sm_get_api_version() < TRUSTY_API_VERSION_MEM_OBJ) {
-        /* If client is not running under a hypervisor allow using old api. */
-        return sm_mem_compat_get_vmm_obj(client_id, mem_obj_id, size, objp,
-                                         obj_ref);
+    if (sm_check_and_lock_api_version(TRUSTY_API_VERSION_MEM_OBJ)) {
+        return sm_mem_get_vmm_obj(client_id, mem_obj_id, tag, size, objp,
+                                  obj_ref);
+    } else {
+        if (client_id == 0 && tag == 0) {
+            /* If client is not running under a hypervisor allow using
+               old api. */
+            return sm_mem_compat_get_vmm_obj(client_id, mem_obj_id, size, objp,
+                                             obj_ref);
+        } else {
+            return ERR_NOT_SUPPORTED;
+        }
     }
-
-    mutex_acquire(&sm_mem_ffa_lock);
-
-    ret = ffa_mem_retrieve((uint16_t)client_id, mem_obj_id, tag, objp, obj_ref);
-
-    mutex_release(&sm_mem_ffa_lock);
-
-    return ret;
 }
 
 /**
-- 
2.17.1

